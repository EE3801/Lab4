# Part 2. Setting up a parallel processing pipeline

## 4.
In Lab 4, we created a serial processing script to process the data. If you have multiple days of data to process, you will be able to use the script to process data from each day in parallel on a cluster. However, if you are processing only one day of data, the time it will take will not be very different from running on your computer. So in this lab, we will look at how we can increase the granularity of the parallelization so we can speed up the processing for even one day of data.

## 5.
Looking at the serial processing pipeline, we notice that there are two groups of objects:
1. RPLParallel object, and all the objects that depend on it: `Unity`, `Eyelink`, `aligning_objects`, and `raycasting`
2. RPLRaw objects, and all the objects that depend on it: `RPLLFP`, `RPLHighPass`, and `spike sorting`. 

This means that these scripts can be run in parallel. 

So create a cluster, login to your cluster, copy your AWS credentials to your home directory, initialize `conda`, activate the `env1` conda environment, change directory to `/data/src/PyHipp`, and edit a copy of the slurm script you created in Lab 4:

```shell
(env1) [ec2-user@ip-10-0-5-43 PyHipp]
$ cp pipeline-slurm.sh rplparallel-slurm.sh

(env1) [ec2-user@ip-10-0-5-43 PyHipp]
$ nano rplparallel-slurm.sh
```

## 6.
We will now remove the commands to create the second group of objects and change the notification message to distinguish it from the second script we will be creating:

```shell
python -u -c "import PyHipp as pyh; \
import DataProcessingTools as DPT; \
import os; \
import time; \
t0 = time.time(); \
print(time.localtime()); \
DPT.objects.processDirs(dirs=None, objtype=pyh.RPLParallel, saveLevel=1); \
DPT.objects.processDirs(dirs=None, objtype=pyh.Unity, saveLevel=1); \
pyh.EDFSplit(); \
os.chdir('session01'); \
pyh.aligning_objects(); \
pyh.raycast(1); \
print(time.localtime()); \
print(time.time()-t0);"

aws sns publish --topic-arn arn:aws:sns:ap-southeast-1:123456789012:awsnotify --message "RPLParallelJobDone"
```

Remember to also edit the following lines in the file to make the job name and the slurm output files more distinct:

```bash
#SBATCH -J "rplpl"   # job name
#SBATCH -o rplpl-slurm.%N.%j.out # STDOUT
#SBATCH -e rplpl-slurm.%N.%j.err # STDERR
```

## 7.
Next, we will create a second copy of pipeline-slurm.sh, name it rplsplit-slurm.sh, and remove the commands to create the first group of objects:

```shell
python -u -c "import PyHipp as pyh; \
import DataProcessingTools as DPT; \
import os; \
import time; \
t0 = time.time(); \
print(time.localtime()); \
DPT.objects.processDirs(dirs=None, objtype=pyh.RPLSplit, channel=[9, 31, 34, 56, 72, 93, 119, 120]); \
DPT.objects.processDirs(dirs=None, objtype=pyh.RPLLFP, saveLevel=1); \
DPT.objects.processDirs(dirs=None, objtype=pyh.RPLHighPass, saveLevel=1); \
os.chdir('session01'); \
DPT.objects.processDirs(level='channel', cmd='import PyHipp as pyh; from PyHipp import mountain_batch; mountain_batch.mountain_batch(); from PyHipp import export_mountain_cells; export_mountain_cells.export_mountain_cells();'); \
print(time.localtime()); \
print(time.time()-t0);"

aws sns publish --topic-arn arn:aws:sns:ap-southeast-1:123456789012:awsnotify --message "RPLSplitJobDone"
```

Remember to also edit the following lines in the file to make the job name and the slurm output files more distinct:

```bash
#SBATCH -J "rplspl"   # job name
#SBATCH -o rplspl-slurm.%N.%j.out # STDOUT
#SBATCH -e rplspl-slurm.%N.%j.err # STDERR
```

