# Part 5. Parallelizing RPLSplit

## 14.
While waiting for the emails notifying you that the jobs have finished, you can skip to the next step. Once the jobs have finished, make a copy of the `checkfiles.sh` shell script named `checkfiles2.sh`, and modify it to produce the following output consisting of the start and end times of each job (in the dash block below):

```shell
(env1) [ec2-user@ip-10-0-14-97 20181105]
$ bash /data/src/PyHipp/checkfiles2.sh 

Number of hkl files
53
Number of mda files
8

#===========================================================
Start Times
==> rplpl-slurm.queue1-dy-m54xlarge-1.4.out <==
time.struct_time(tm_year=2021, tm_mon=10, tm_mday=15, tm_hour=14, tm_min=45, tm_sec=44, tm_wday=4, tm_yday=288, tm_isdst=0)

==> rplspl-slurm.queue1-dy-m54xlarge-1.5.out <==
time.struct_time(tm_year=2021, tm_mon=10, tm_mday=15, tm_hour=14, tm_min=45, tm_sec=44, tm_wday=4, tm_yday=288, tm_isdst=0)
End Times
==> rplpl-slurm.queue1-dy-m54xlarge-1.4.out <==
time.struct_time(tm_year=2021, tm_mon=10, tm_mday=15, tm_hour=15, tm_min=16, tm_sec=6, tm_wday=4, tm_yday=288, tm_isdst=0)
1821.3657758235931
{
    "MessageId": "ebc92db5-3ca8-5f1d-9760-7456a1df4587"
}

==> rplspl-slurm.queue1-dy-m54xlarge-1.5.out <==
time.struct_time(tm_year=2021, tm_mon=10, tm_mday=15, tm_hour=15, tm_min=36, tm_sec=16, tm_wday=4, tm_yday=288, tm_isdst=0)
3031.305139541626
{
    "MessageId": "e02b2fcc-4606-51de-9dd1-762fe4377c35"
}
#===========================================================
```

You can now add the time taken for each job to compute the total time it would have taken to run the two jobs serially. In order to compute the actual time taken for both jobs to complete, take the difference between the earlier of the two start times and the later of the two end times. Take the difference between the two durations to compute how much time the parallel processing saved. Compute also how much time it would have taken to process 110 channels of data. When extrapolating the time taken for 110 channels, remember to extrapolate only the time taken for the second job since the first job will only be run once regardless of the number of channels processed.

> <p class="task"> Task
>
> Include in your lab report (for Part B) a screenshot of your `checkfiles2.sh` script and its output. 
> Include also:
> 
> a. the sum of the two job durations in hours, minutes, and seconds, 
> 
> b. the actual time taken for both jobs to complete, 
> 
> c. the time saved in parallelizing the jobs
> 
> d. the time it will take to process all 110 channels. 

## 15.
In the previous setup, we used a single job to split the channels in the `.ns5` file serially. One way to speed up the processing will be to use multiple jobs to process different groups of channels so the channels can be split in parallel. One way to do this is to process the channels by arrays of 32 channels.

## 16.
Make a copy of `rplsplit-slurm.sh`, and edit it using nano: 

```shell
(env1) [ec2-user@ip-10-0-5-43 20181105]
$ cd -

(env1) [ec2-user@ip-10-0-5-43 PyHipp]
$ cp rplsplit-slurm.sh rs1-slurm.sh

(env1) [ec2-user@ip-10-0-5-43 PyHipp]
$ nano rs1-slurm.sh
```

## 17.
Modify the commands in `rs1-slurm.sh` to the following:

```bash
DPT.objects.processDirs(dirs=None, objtype=pyh.RPLSplit, channel=[*range(1,33)]);
```

If called from the day directory, this will make `RPLSplit` split and save any channels in the range of 1 to 32 that it finds in the `.ns5` file in both the `sessioneye` and `session01` directories.

## 18.
Once the `rplraw` files have been created in the channel directories, we would want to convert the files in the first array to `rpllfp` and `rplhighpass` files, so we will need to add the following commands:

```bash
DPT.objects.processDirs(dirs=['sessioneye/array01','session01/array01'], cmd='import PyHipp as pyh; import DataProcessingTools as DPT; DPT.objects.processDirs(None, pyh.RPLLFP, saveLevel=1); DPT.objects.processDirs(None, pyh.RPLHighPass, saveLevel=1);');
```

Note that specifying `dirs=['sessioneye/array01','session01/array01']` is yet another way to use the `processDirs` function, which specifies the directories you want to run a particular command, or create an object. Specifying the `array01` directories in this way ensures that this slurm job only processes the channels in the first array directory, and not the channels in the other array directories, which we would want other jobs to handle.

## 19.
The spike sorting script will combine the highpass files in both the `sessioneye` and `session01` directories before starting the spike sorting process, but again we need to call it from the `session01/array01` directory to process only the channels in the first array: 

```bash
os.chdir('session01/array01'); DPT.objects.processDirs(level='channel', cmd='import PyHipp as pyh; from PyHipp import mountain_batch; mountain_batch.mountain_batch(); from PyHipp import export_mountain_cells; export_mountain_cells.export_mountain_cells();');
```

## 20.
The python command in the modified file should look like:

```bash
python -u -c "import PyHipp as pyh; \
import DataProcessingTools as DPT; \
import os; \
import time; \
t0 = time.time(); \
print(time.localtime()); \
DPT.objects.processDirs(dirs=None, objtype=pyh.RPLSplit, channel=[*range(1,33)]); \
DPT.objects.processDirs(dirs=['sessioneye/array01','session01/array01'], cmd='import PyHipp as pyh; import DataProcessingTools as DPT; DPT.objects.processDirs(None, pyh.RPLLFP, saveLevel=1); DPT.objects.processDirs(None, pyh.RPLHighPass, saveLevel=1);'); \
os.chdir('session01/array01'); \
DPT.objects.processDirs(level='channel', cmd='import PyHipp as pyh; from PyHipp import mountain_batch; mountain_batch.mountain_batch(); from PyHipp import export_mountain_cells; export_mountain_cells.export_mountain_cells();'); \
print(time.localtime()); \
print(time.time()-t0);"
```

Remember to also edit the following lines in the file to make the job name and the slurm output files more distinct:

```bash
#SBATCH -J "rs1"   # job name
#SBATCH -o rs1-slurm.%N.%j.out # STDOUT
#SBATCH -e rs1-slurm.%N.%j.err # STDERR
```

Change the AWS SNS notification message so that it is unique to this script. 

## 21.
Create scripts to process the other three arrays where the array directory and channel ranges should be:

```bash
array02 channel=[*range(33,65)]
array03 channel=[*range(65,97)]
array04 channel=[*range(97,125)]
```

## 22.
Instead of having to submit these five scripts manually, we can create a script to submit the jobs:

```shell
(env1) [ec2-user@ip-10-0-5-43 PyHipp]
$ nano pipe2.sh
```

Enter the following into the file:

```bash
#!/bin/bash

# first job called from the day directory
# creates RPLParallel, Unity, and EDFSplit objects, and
# calls aligning_objects and raycast
sbatch /data/src/PyHipp/rplparallel-slurm.sh

# second set of jobs called from the day directory
sbatch /data/src/PyHipp/rs1-slurm.sh
sbatch /data/src/PyHipp/rs2-slurm.sh
sbatch /data/src/PyHipp/rs3-slurm.sh
sbatch /data/src/PyHipp/rs4-slurm.sh
```

## 23.
Remove the files created during the last run in Step 14:

```shell
(env1) [ec2-user@ip-10-0-5-43 PyHipp]
$ cd -

(env1) [ec2-user@ip-10-0-5-43 20181105]
$ bash /data/src/PyHipp/removefiles.sh
```

## 24.
Run `pipe2.sh`:

```shell
(env1) [ec2-user@ip-10-0-5-43 20181105]
$ bash /data/src/PyHipp/pipe2.sh
```

## 25.
Check the queue to see how the jobs have been assigned to the compute nodes (it might take some time for the compute node to start up and for the `ST` (status) column to change from `CF` (configuring) to `R` (running)):


```shell
JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
    2   queue1    rplpl ec2-user  R       0:02      1 queue1-dy-r52xlarge-1 
    3   queue1      rs1 ec2-user  R       0:02      1 queue1-dy-r52xlarge-1 
    4   queue1      rs2 ec2-user  R       0:02      1 queue1-dy-r52xlarge-1 
    5   queue1      rs3 ec2-user  R       0:02      1 queue1-dy-r52xlarge-1 
    6   queue1      rs4 ec2-user  R       0:02      1 queue1-dy-r52xlarge-1
```

## 26.
As you can see, all five jobs are running on one compute node (`queue1-dy-r52xlarge-1` in this example but your instance type will depend on what you specified in your `cluster-config.yaml` file). This is not ideal as each of the `rplsplit` jobs will be reading in the `.ns5` files, and will require approximately 40 GB of memory. The compute node only has 64 GB of memory, so we can either choose a larger instance type with more memory, or we can modify the slurm script to create more compute nodes so that each of the `rplsplit` jobs can be run on one compute node. We will now do the latter.

## 27.
You can go ahead and cancel the jobs (replace the job ids below with those from your queue):

```shell
(env1) [ec2-user@ip-10-0-5-43 20181105]
$ scancel {2..7}
```

## 28.
Remove the files created during the last run.

## 29.
Edit each of the `rs?-slurm.sh` scripts to add the following line (under the line `#SBATCH --nodes=1`) if you are using a `2xlarge` instance type with 8 vCPUs for your compute node (e.g. `z1d.2xlarge`, `r5dn.2xlarge`, etc.):

```bash
#SBATCH --cpus-per-task=5   # number of CPUs for this task
```

and the following line if you are using a `4xlarge` instance type with 16 vCPUs (e.g. `m5.4xlarge`, `m5a.4xlarge`, etc.):

```bash
#SBATCH --cpus-per-task=10   # number of CPUs for this task
```

If a compute node instance type we are using has 8 vCPUs (e.g. `z1d.2xlarge`), once one of the compute nodes is assigned one of the rplsplit jobs that requires 5 vCPUs, there will only be 3 vCPUs left. This will be insufficient to assign another rplsplit job, so another compute node will be started up to run the next job. This will allow all 64 GB of memory in a node to be dedicated to one job instead of being shared between multiple jobs. Using `cpus-per-task=5` instead of `cpus-per-task=8` allows other jobs that do not require a lot of memory (e.g. `rplparallel-slurm.sh`) to run concurrently on one of the compute nodes.

## 30.
Re-run `pipe2.sh`.

## 31.
Check the queue to see how the jobs have been assigned to the compute nodes (it might take some time for the compute nodes to start up):

```shell
JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) 
    7   queue1    rplpl ec2-user  R       4:31      1 queue1-dy-r52xlarge-1 
    8   queue1      rs1 ec2-user  R       4:31      1 queue1-dy-r52xlarge-1 
    9   queue1      rs2 ec2-user  R       1:25      1 queue1-dy-r52xlarge-2 
   10   queue1      rs3 ec2-user  R       1:25      1 queue1-dy-r52xlarge-3 
   11   queue1      rs4 ec2-user  R       1:25      1 queue1-dy-r52xlarge-4 

```

## 32.
As you can see, the `rs1`, `rs2`, `rs3`, and `rs4` jobs are now running on different compute nodes (`queue1-dy-r52xlarge-1`, `queue1-dy-r52xlarge-2`, `queue1-dy-r52xlarge-3`, `queue1-dy-r52xlarge-4`). Increasing the memory requirement by increasing the number of vCPUs required allowed the jobs to run successfully in parallel.

## 33.
You can now cancel the jobs. Update your snapshot using another Terminal window so the changes are saved for use later in this lab.
