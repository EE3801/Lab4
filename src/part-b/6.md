# Part 6. Automatically deleting the head node

## 34.
Before we start to process all the channels, we need to do one more thing. Although the compute nodes will be terminated when all the jobs are completed, the head node will continue to run, which will consume your AWS credits. So we are going to modify your setup that will allow you to cleanly terminate your head node once all the jobs are completed. This will involve using a small EC2 instance that can be supported by your AWS free-tier credits to initiate a snapshot of your cluster, and then use AWS Lambda to shut down your cluster once the snapshot has been completed. You can find more about AWS Lambda, which allows you to write code to interface with your AWS services [here](https://aws.amazon.com/lambda/).

## 35.
First, open up a new terminal window on your computer, activate your aws environment, and create two new clusters named  `testcluster1` and `testcluster2`.

## 36.
Next, change directories to the directory on your computer that contains the file `MyKeyPair.pem`. Then, do the following (which is similar to what you did in Lab 3) to create a small EC2 instance:

```shell
(aws) 
$ aws ec2 run-instances --image-id resolve:ssm:/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2 --instance-type t2.micro --key-name MyKeyPair
```

This will return something that looks like this:

```shell
{
    "Groups": [],
    "Instances": [
        {
            "AmiLaunchIndex": 0,
            "ImageId": "ami-0cd31be676780afa7",
            "InstanceId": "i-047ed6d17f231355d",
            "InstanceType": "t2.micro",
            "KeyName": "MyKeyPair",
            "LaunchTime": "2020-08-09T14:17:12+00:00",
            "Monitoring": {
                "State": "disabled"
            },
```

Take note of the InstanceId, as highlighted above. You can hit the space bar to scroll through the rest of the information returned by the function, and then hit `q` when you get to the end, or you can hit `q` at any time to get out of the remainder of the display.

## 37.
While waiting for your clusters and EC2 instance to be set up, you can set up your security settings to enable Lambda. Open the AWS Management Console in your web browser, type `IAM` in the `Services` search bar, click on `Policy` in the sidebar, click the `Create Policy` button, click the `JSON` tab, and replace the code with:

```shell
{
    "Version": "2012-10-17",
    "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "ec2:Start*",
        "ec2:Stop*",
  "ec2:Terminate*"
      ],
      "Resource": "*"
    }
    ]
}
```

Click the `Next: Tags` button in the bottom-right, click the `Next: Review` button on the next page, enter `LambdaPolicy` in the `Name` textfield, and then click the `Create policy` button. 

## 38.
Next, click on `Roles` in the sidebar, click on the `Create role` button, select `Lambda` under `Common use cases`, click the `Next: Permissions` button, type `LambdaPolicy` in the search bar, scroll down to locate `LambdaPolicy`, click the checkbox for `LambdaPolicy`, click the `Next: Tags` button, enter `LambdaRole` in `Role name`, and then click the `Create role` button.

## 39.
We now need to find the `InstanceID` for the head node of the first test cluster we just created. You can find this by typing `EC2` in the `Services` search bar, and click on `Instances` in the sidebar. Scroll to the right to find the cluster with `testcluster1` in its entry under the `Security group name` column. Click on the row, and then click on the icon beside the Instance ID in the lower half of the window to copy it to your clipboard.

Your small EC2 instance should be up and running now as well, so note down its Public IPv4 address as you will be using this EC2 instance for the rest of the module. You should also note down the `InstanceIDs` and the Public IPv4 addresses for the head node of the second test cluster, as well as the head node of your first cluster, i.e. `MyCluster01`.

## 40.
Now we will create an AWS Lambda function that will shut down the head node. You can type `Lambda` in the `Services` search bar, click on `Functions` in the sidebar, click on the `Create function` button, and follow the instructions in the `Create Lambda functions that stop and start your EC2 instances` section in the link Stop and start EC2 instances at set intervals using Lambda (amazon.com). However, change your region to `ap-southeast-1`, replace the example EC2 instance IDs with the ID you copied above, and replace the following code:

```python
ec2.stop_instances(InstanceIds=instances)
print('stopped your instances: ' + str(instances))
```

with:

```python
ec2.terminate_instances(InstanceIds=instances)
print('terminated your instances: ' + str(instances))
```

You want to terminate instances instead of stopping them as you will still be charged for the volumes attached to instances that are stopped. Click the `Deploy` button to save your function.

Click the `Test` button to check that you were able to terminate the head node of the first cluster. You should see messages in the `Execution Results` tab that reflect the outcome of the function.

## 41.
Although terminating the head node stops you from being charged, the cluster still exists as you can see with the following command:

```shell
(aws) 
$ pcluster list-clusters --region ap-southeast-1
```

This will prevent you from creating another cluster in the future with the same name. So you will still need to delete the cluster by doing:

```shell
(aws) 
$ pcluster delete-cluster --region ap-southeast-1 --cluster-name testcluster1
```

## 42.
Replace the Instance ID in the `instances` variable in the `lambda_function` tab with that of the second test cluster. Click the `Deploy` button to save your changes.

## 43.
Now you can use **EventBridge** to set up a new rule that will execute your Lambda function when the snapshot is completed. 

Follow the instructions in Lab 3 to
- select `EC2` for `Service Name` 
- `EBS Snapshot Notification` for `Event Type` 
- `createSnapshot` for `Specific event(s)`
- `succeeded` for `Specific result(s)`

Click the `Next` button, and then under the `Target 1` section, makes sure `AWS Service` is selected, select `Lambda function`, followed by the name of the function you created under `Functions`. 

Click the `Add another target*` button in the bottom left to also send you an email by selecting `SNS topic` from the first drop-down list and `awsnotify` from the Topic drop-down list. 

Expand the `Additional settings` section, select the `Constant (JSON text)` option, and enter `ClusterTerminated` (with the quotes included). This will include the text `ClusterTerminated` in the email message sent by awsnotify. 

Click the `Next` button twice to go through the next two screens, and then click the `Create rule` button.

## 44.
In order to avoid receiving two emails notifying you that the next snapshot has been completed successfully, select the `SnapshotComplete` rule (or whatever you named the rule in Lab 4), select the `Actions` drop-down menu and click on `Disable`.

## 45.
Use the IP address of your small EC2 instance (described in [Step 39](6.md#39)) to copy your AWS credentials, the pcluster configuration file, your key pair, and the `update_snapshot.sh` script from your computer to the EC2 instance:

```shell
(aws) 
$ scp -rp -i ./MyKeyPair.pem ~/.aws ~/cluster-config.yaml ./MyKeyPair.pem ~/Documents/Python/PyHipp/update_snapshot.sh ec2-user@54.169.221.196:~/
```

> <p class="warn"> Warning
>
> Replace the `54.169.221.196` with your own ip address

## 46.
Use the address returned above to ssh into your EC2 instance:

```shell
(aws) 
$ ssh -i ./MyKeyPair.pem ec2-user@54.169.221.196
```

> <p class="warn"> Warning
>
> Replace the `54.169.221.196` with your own ip address

## 47.
Check that your AWS credentials are set up properly by sending a notification message:

```shell
[ec2-user@ip-54.169.221.196 ~]
$ aws sns publish --topic-arn arn:aws:sns:ap-southeast-1:xxxxxx:awsnotify --message "FromEC2"
```

> <p class="warn"> Warning
>
> Replace the `xxxxxx` with your own key

## 48.
Since we will be creating snapshots from this EC2 instance, which will update the cluster config file saved here with the latest snapshot ID, we will need to create clusters from here from now on. Once you are logged in, follow the instructions [here](https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-pip.html) to install ParallelCluster.

If you get an error while running the command 

```shell
nvm install --lts
```

you can try the following command instead:

```bash
nvm install 16.9.1
```

## 49.
Now ssh to `testcluser2` by doing:

```shell
[ec2-user@ip-54.169.221.196 ~]
$ pcluster ssh -i ~/MyKeyPair.pem --region ap-southeast-1 --cluster-name testcluster2 
```

Remember to perform the usual initialization of the cluster.

## 50.
We will now create a simple test to check that the head node for `testcluster2` can be successfully deleted after a snapshot has been completed. First we will create a simple slurm job that will just wait for 30 seconds. We can do this by making a copy of the slurm.sh file in the PyHipp directory, and adding the following lines to the end of the file (similar to what you did in Lab 4):

```bash
sleep 30

aws sns publish --topic-arn arn:aws:sns:ap-southeast-1:xxxxxx:awsnotify --message "SleepJobDone"
```

> <p class="warn"> Warning
>
> Replace the `xxxxxx` with your own key

## 51.
As we discussed in the lecture, the `consol_jobs.sh` script is used to wait for all the jobs to be completed before calling the `ec2snapshot.sh` script that will ssh to the EC2 instance to initiate the snapshot. So for this test, we will make some small changes to the `ec2snapshot.sh` script to make sure everything works as expected. 

```shell
(env1) [ec2-user@ip-10-0-8-173 PyHipp]
$ cat ec2snapshot.sh 

#!/bin/bash

ssh -o StrictHostKeyChecking=no -i /data/MyKeyPair.pem ec2-user@xx.xx.xx.xx "source ~/.bash_profile; pcluster update-compute-fleet --status STOP_REQUESTED --region ap-southeast-1 --cluster-name MyCluster01; ~/update_snapshot.sh data 2 MyCluster01"
```

Edit the ec2snapshot.sh file found in the /data/src/PyHipp directory to replace the `xx.xx.xx.xx` with the Public IP address for your EC2 instance. Also replace `MyCluster01` with `testcluster2`, and `data` (the name for your snapshot) with `testdata`.

## 52.
As you can see above, the script sends several commands over ssh to your EC2 instance to be executed. So it will need a copy of your key pair file in order to access your EC2 instance. You can transfer the key pair file from your computer to `testcluster2` by doing (replace the IP address below with the public IP address of `testcluster2`):

```shell
(aws) 
$ scp -i ~/MyKeyPair.pem ~/MyKeyPair.pem ec2-user@54.255.166.210:/data
```

Note that we are saving the key pair file in `/data` so it will be saved in subsequent snapshots.

## 53.
You can now submit the simple slurm job you created above (replace the name below with the name of your script):

```shell
(env1) [ec2-user@ip-10-0-8-173 PyHipp]
$ sbatch sleepslurm.sh 
```

Take note of the job number that is returned by slurm.

## 54.
You can now submit the `consol_jobs.sh` script to depend on the previous job (replace `2` with the job number from the preceding step):

```shell
(env1) [ec2-user@ip-10-0-8-173 PyHipp]
$ sbatch --dependency=afterok:2 /data/src/PyHipp/consol_jobs.sh
```

## 55.
After the first job completes, the second job will call `ec2snapshot.sh` to stop the compute nodes and initiate a snapshot from your EC2 instance. After the snapshot is complete, the head node will be terminated, at which point, you will be logged out of your cluster and returned back to the prompt on your EC2 instance. At this point, you can now call pcluster to delete `testcluster2`. At this point, you can also go to your AWS Management Console in your browser, and delete the `testdata` snapshot.

If you have trouble getting the `ec2snapshot.sh` script to work, you can add the full path to the pcluster function in the script:

```bash
#!/bin/bash

ssh -o StrictHostKeyChecking=no -i /data/MyKeyPair.pem ec2-user@xx.xx.xx.xx "source ~/.bash_profile; /home/ec2-user/.local/bin/pcluster update-compute-fleet --status STOP_REQUESTED --region ap-southeast-1 --cluster-name MyCluster01; ~/update_snapshot.sh data 2 MyCluster01"
```

## 56. 
You can now copy your key pair from your EC2 instance to `/data` on your `MyCluster01` cluster, ssh to your `MyCluster01` cluster from your EC2 instance, modify the `ec2snapshot.sh` script to enter the IP address of your EC2 instance. Check that the name of the cluster is `MyCluster01` and the name of the snapshot is `data`. You will also need to change the instance ID in your Lambda function to the one for `MyCluster01`.