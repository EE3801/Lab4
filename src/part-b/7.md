# Part 7. Executing the parallel processing pipeline

## 57.
You are now ready to process all the channels. Return to the `20181105` directory, remove files generated by the previous job, and re-run `pipe2.sh`.


## 58.
Check that the jobs were submitted successfully, and then submit the `consol_jobs.sh` script with a dependence on the five jobs submitted by the `pipe2.sh` script (replace the job numbers with those in your queue): 

```shell
(env1) [ec2-user@ip-10-0-5-43 20181105] $ sbatch --dependency=afterok:12:13:14:15:16 /data/src/PyHipp/consol_jobs.sh
```

## 59.
Check the queue to make sure everything is set up properly:

```shell
JOBID PARTITION       NAME         USER   ST      TIME    NODES NODELIST(REASON) 
  12   queue1        rplpl     ec2-user    R      4:31      1   queue1-dy-r52xlarge-1 
  13   queue1          rs1     ec2-user    R      4:31      1   queue1-dy-r52xlarge-1 
  14   queue1          rs2     ec2-user    R      1:25      1   queue1-dy-r52xlarge-2 
  15   queue1          rs3     ec2-user    R      1:25      1   queue1-dy-r52xlarge-3 
  16   queue1          rs4     ec2-user    R      1:25      1   queue1-dy-r52xlarge-4 
  17   queue1     consol_j     ec2-user   PD      0:00      1   (Dependency)
```

## 60.
You can now exit the cluster, and check on the progress either from the EC2 instance (described in [Step 55](6.md#55)) or from your computer:

```shell
(aws) $ pcluster ssh -i ~/MyKeyPair.pem -n MyCluster01 squeue
```

This command will allow you to run the squeue command on your cluster, and get the output, without actually logging into the cluster.